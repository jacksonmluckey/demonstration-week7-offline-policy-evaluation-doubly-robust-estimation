{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "adb22bf8",
      "metadata": {},
      "source": [
        "#### **Week 7 Demo: Offline Policy Evaluation with Doubly Robust Estimation**\n",
        "\n",
        "**Course:** Causal Machine Learning (GRAD-E1487), Fall 2025  \n",
        "**Instructor:** Drew Dimmery  \n",
        "**Presentors:** Aditi, Emilie\n",
        "**Date:** October 21, 2025  \n",
        "\n",
        "\n",
        "#### I. Introduction & Motivation  \n",
        "#### From Effect Estimation to Policy Evaluation  \n",
        "\n",
        "What we've done so far in this course:\n",
        "\n",
        "- Estimated treatment effects:  \n",
        "  $$\n",
        "  \\tau = \\mathbb{E}[Y(1) - Y(0)]\n",
        "  $$\n",
        "\n",
        "- Estimated conditional treatment effects:  \n",
        "  $$\n",
        "  \\tau(x) = \\mathbb{E}[Y(1) - Y(0) \\mid X = x]\n",
        "  $$\n",
        "\n",
        "- **Goal:** Understand how large effects are (and for whom)\n",
        "\n",
        "\n",
        "\n",
        "#### Today's shift:\n",
        "\n",
        "- **Evaluate and learn policies (decision rules)**  \n",
        "- A policy $\\pi: \\mathcal{X} \\to \\{0,1\\}$ maps covariates to treatment assignments  \n",
        "- **Policy value:**  \n",
        "  $$\n",
        "  V(\\pi) = \\mathbb{E}[Y(\\pi(X))]\n",
        "  $$\n",
        "  expected outcome if we follow $\\pi$  \n",
        "- **Goal:** Find policies that produce good outcomes, not just estimate effects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f86d15e4",
      "metadata": {},
      "source": [
        "#### Why This Matters\n",
        "\n",
        "- Decision-making requires choosing who gets treated (not just knowing effect sizes)  \n",
        "- Can evaluate policies without estimating $\\tau(x)$ for all $x$  \n",
        "\n",
        "\n",
        "\n",
        "##### II. The Challenge: Off-Policy Evaluation  \n",
        "### The Contextual Bandit Setting  \n",
        "\n",
        "**Data structure:**\n",
        "\n",
        "- Logged data from a behavior policy $\\pi_0$: $(X_i, A_i, Y_i)$ for $i = 1, ..., n$  \n",
        "- Treatments assigned by $\\pi_0$: $A_i \\sim \\pi_0(\\cdot \\mid X_i)$  \n",
        "- Partial feedback: Only observe outcome for chosen action: $Y_i = Y_i(A_i)$  \n",
        "\n",
        "\n",
        "**The goal:**\n",
        "\n",
        "- Evaluate a target policy $\\pi$ without deploying it  \n",
        "- Estimate:  \n",
        "  $$\n",
        "  V(\\pi) = \\mathbb{E}_X\\big[\\mathbb{E}[Y(\\pi(X)) \\mid X]\\big]\n",
        "  $$\n",
        "- But data came from $\\pi_0$, not $\\pi$!\n",
        "\n",
        "\n",
        "#### The Critical Requirement: Overlap  \n",
        "\n",
        "- **Overlap assumption:** $\\pi_0(a \\mid x) > 0$ whenever $\\pi(a \\mid x) > 0$  \n",
        "- Without overlap: no data to evaluate $\\pi$ in some regions of $\\mathcal{X}$  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6559b3c0",
      "metadata": {},
      "source": [
        "#### III. Three Estimation Approaches\n",
        "\n",
        " **1. Direct Method (DM)**\n",
        "\n",
        "**Strategy:** Model the outcomes  \n",
        "\n",
        "**Approach:**\n",
        "\n",
        "Estimate  \n",
        "$$\n",
        "\\hat{\\mu}(a, x) = \\mathbb{E}[Y \\mid A = a, X = x]\n",
        "$$\n",
        "\n",
        "Predict:  \n",
        "$$\n",
        "\\hat{V}_{DM}(\\pi) = \\mathbb{E}_X[\\hat{\\mu}(\\pi(X), X)]\n",
        "$$\n",
        "\n",
        "\n",
        "**2. Inverse Propensity Score (IPS)**\n",
        "\n",
        "**Strategy:** Reweight the observations  \n",
        "\n",
        "**Formula:**  \n",
        "$$\n",
        "\\hat{V}_{IPS}(\\pi) = \\frac{1}{n} \\sum_i\n",
        "\\left[\n",
        "\\frac{\\mathbb{1}(A_i = \\pi(X_i))}{\\pi_0(A_i \\mid X_i)} \\cdot Y_i\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "\n",
        "**3. Doubly Robust (DR)**\n",
        "\n",
        "**Strategy:** Combine both approaches  \n",
        "\n",
        "**Formula:**\n",
        "$$\n",
        "\\hat{V}_{DR}(\\pi)\n",
        "= \\frac{1}{n} \\sum_i\n",
        "\\Big[\n",
        "\\hat{\\mu}(\\pi(X_i), X_i)\n",
        "+ \\frac{\\mathbb{1}(A_i = \\pi(X_i))}{\\pi_0(A_i \\mid X_i)} \\cdot (Y_i - \\hat{\\mu}(A_i, X_i))\n",
        "\\Big]\n",
        "$$\n",
        "\n",
        "\n",
        "**Intuition:** Outcome model prediction + importance-weighted residual correction  \n",
        "\n",
        "\n",
        "##### VI. From Evaluation to Learning\n",
        "\n",
        "**So far:** Evaluate a given policy $\\pi$\n",
        "- Given a candidate policy, estimate its value $V(\\pi)$  \n",
        "- Tells us if a proposed policy is good or bad  \n",
        "\n",
        "**The learning problem:** Find the best policy from data  \n",
        "\n",
        "\n",
        "$$\n",
        "\\hat{\\pi} \\;=\\; \\arg\\max_{\\pi \\in \\Pi} \\; \\hat{V}_{DR}(\\pi)\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1e7a568",
      "metadata": {},
      "source": [
        "#### **Demo Structure**\n",
        "\n",
        " **Part 1: Data Generation**\n",
        "\n",
        "- Load a **multiclass classification dataset** (e.g., *digits*, *letter recognition*).  \n",
        "- Implement the **Dudík transformation**:\n",
        "  - For each example, sample an action $A_i \\sim \\pi_0(\\cdot \\mid X_i)$.\n",
        "  - Reveal reward $Y_i = \\mathbb{1}(A_i = c_i)$.\n",
        "  - Store the logging probability $\\pi_0(A_i \\mid X_i)$.  \n",
        "- Create **logging policies** $\\pi_0$ with different exploration levels (*ε*-greedy):\n",
        "  - Mix deterministic and random behavior using ε ∈ {0.1, 0.3, 0.5}.\n",
        "\n",
        " **Part 2: Policy Learning**\n",
        "\n",
        "- Train an **evaluation policy** $\\pi_e$ using supervised learning on the full labels.  \n",
        "- Evaluate how $\\pi_e$ performs under different logging policies $\\pi_0$ with varying exploration levels:\n",
        "  $$\n",
        "  \\varepsilon \\in \\{0.1, 0.3, 0.5\\}\n",
        "  $$\n",
        "\n",
        " **Part 3: Estimation Implementation**\n",
        "\n",
        "- Implement the **Direct Method (DM)** estimator.  \n",
        "- Implement the **Inverse Propensity Score (IPS)** estimator.  \n",
        "- (Later) add the **Doubly Robust (DR)** estimator for comparison.\n",
        "- Implement **DR estimator**  \n",
        "- Run **all three estimators** on the same data\n",
        "\n",
        " **Part 4: Simulation & Analysis**\n",
        "\n",
        "- Run 100+ simulation replications  \n",
        "- Compute for each estimator:\n",
        "\n",
        "  - **Bias:**  \n",
        "    $$\n",
        "    \\mathbb{E}[\\hat{V}] - V_{\\text{true}}\n",
        "    $$\n",
        "\n",
        "  - **Variance:**  \n",
        "    $$\n",
        "    \\operatorname{Var}[\\hat{V}]\n",
        "    $$\n",
        "\n",
        "  - **MSE:**  \n",
        "    $$\n",
        "    \\text{Bias}^2 + \\text{Variance}\n",
        "    $$\n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38089a40",
      "metadata": {},
      "source": [
        "# ADEMP Simulation\n",
        "\n",
        "## A - AIM\n",
        "Demonstrate that doubly robust (DR) estimators provide more reliable offline policy evaluation than Direct Method (DM) or Inverse Propensity Scoring (IPS) alone.\n",
        "\n",
        "**Research Questions:**\n",
        "\n",
        "- Can we accurately estimate the true policy value \\(V(\\pi_e)\\) using only logged data?  \n",
        "- How does estimation error change when the reward model is misspecified?  \n",
        "- How does estimation error change when overlap decreases (poor exploration)?  \n",
        "- How do DM, IPS, and DR compare in terms of bias, variance, and MSE?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db58976d",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8e99db",
      "metadata": {},
      "outputs": [],
      "source": [
        "## D - DATA GENERATING PROCESS\n",
        "\n",
        "# Loading necessary Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90495b72",
      "metadata": {},
      "outputs": [],
      "source": [
        "### 1. Load UCI Letters Dataset\n",
        "\n",
        "# Letters dataset has 26 classes (A-Z), 16 features\n",
        "# Using sklearn's fetch_openml to get the dataset\n",
        "letters = fetch_openml('letter', version=1, parser='auto')\n",
        "X = letters.data.values if hasattr(letters.data, 'values') else letters.data\n",
        "y = letters.target.values if hasattr(letters.target, 'values') else letters.target\n",
        "\n",
        "# Encode labels to integers 0-25\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "n_classes = len(np.unique(y_encoded))\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "print(f\"Class distribution:\\n{pd.Series(y).value_counts().sort_index()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8513251",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b3c431",
      "metadata": {},
      "outputs": [],
      "source": [
        "### 2. Train-Test Split (50-50 split as in paper)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32acff08",
      "metadata": {},
      "outputs": [],
      "source": [
        "### 3. Create Fully Labeled Dataset\n",
        "# Training data: fully revealed (all losses visible)\n",
        "train_losses_full = np.zeros((len(X_train), n_classes))\n",
        "for i, true_class in enumerate(y_train):\n",
        "    for a in range(n_classes):\n",
        "        train_losses_full[i, a] = 1 if a != true_class else 0  # 0-1 loss\n",
        "\n",
        "# Test data: fully revealed (for ground truth computation)\n",
        "test_losses_full = np.zeros((len(X_test), n_classes))\n",
        "for i, true_class in enumerate(y_test):\n",
        "    for a in range(n_classes):\n",
        "        test_losses_full[i, a] = 1 if a != true_class else 0\n",
        "\n",
        "print(\"\\nFully labeled data created.\")\n",
        "print(f\"Train losses shape: {train_losses_full.shape}\")\n",
        "print(f\"Test losses shape: {test_losses_full.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec0daa3",
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4. Function to Convert to Bandit Feedback\n",
        "def create_bandit_data(X, y, losses_full, logging_policy='uniform', epsilon=None):\n",
        "    \"\"\"\n",
        "    Transform fully labeled data into bandit feedback (partially labeled).\n",
        "    \"\"\"\n",
        "    n_samples, n_classes = losses_full.shape\n",
        "    actions = np.zeros(n_samples, dtype=int)\n",
        "    propensities = np.zeros(n_samples)\n",
        "    \n",
        "    #simulating logging policy for uniform case - for each example randomly selecting one of the 26 letters uniformly\n",
        "    if logging_policy == 'uniform':\n",
        "        # Uniform random logging: p(a|x) = 1/k for all a\n",
        "        actions = np.random.choice(n_classes, size=n_samples)\n",
        "        propensities = np.ones(n_samples) / n_classes\n",
        "\n",
        "    #simulating logging policy with epsilon greedy    \n",
        "    elif logging_policy == 'epsilon_greedy':\n",
        "        # Epsilon-greedy: with prob epsilon select random, else select true class\n",
        "        if epsilon is None:\n",
        "            epsilon = 0.1\n",
        "        \n",
        "        for i in range(n_samples):\n",
        "            if np.random.rand() < epsilon:\n",
        "                # Explore: random action\n",
        "                actions[i] = np.random.choice(n_classes)\n",
        "                propensities[i] = epsilon / n_classes + (1 - epsilon) * (actions[i] == y[i])\n",
        "            else:\n",
        "                # Exploit: select true class (deterministic logging policy)\n",
        "                actions[i] = y[i]\n",
        "                propensities[i] = epsilon / n_classes + (1 - epsilon)\n",
        "    \n",
        "    # Extract observed losses and rewards for selected actions\n",
        "    # This is creating partial feedback as it is only extracting loss and rewards for the selected action\n",
        "    losses_observed = losses_full[np.arange(n_samples), actions]\n",
        "    rewards_observed = 1 - losses_observed  # Convert loss to reward\n",
        "    \n",
        "    bandit_data = {\n",
        "        'contexts': X,\n",
        "        'actions': actions,\n",
        "        'rewards': rewards_observed,\n",
        "        'losses': losses_observed,\n",
        "        'propensities': propensities,\n",
        "        'true_labels': y\n",
        "    }\n",
        "    \n",
        "    return bandit_data\n",
        "\n",
        "# Example: Create bandit feedback with uniform logging policy\n",
        "print(\"\\n Creating Bandit Feedback (Uniform Logging)\")\n",
        "test_bandit = create_bandit_data(X_test, y_test, test_losses_full, logging_policy='uniform')\n",
        "\n",
        "print(f\"Bandit data created:\")\n",
        "print(f\"  - Contexts shape: {test_bandit['contexts'].shape}\")\n",
        "print(f\"  - Actions shape: {test_bandit['actions'].shape}\")\n",
        "print(f\"  - Propensities (first 5): {test_bandit['propensities'][:5]}\")\n",
        "print(f\"  - Average propensity: {test_bandit['propensities'].mean():.4f}\")\n",
        "print(f\"\\nFirst example:\")\n",
        "print(f\"  - True label: {test_bandit['true_labels'][0]}\")\n",
        "print(f\"  - Logged action: {test_bandit['actions'][0]}\")\n",
        "print(f\"  - Observed loss: {test_bandit['losses'][0]}\")\n",
        "print(f\"  - Propensity: {test_bandit['propensities'][0]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0ba05d",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Step 2: TRAIN THE EVALUATION POLICY π (on training data with full feedback)\n",
        "policy = LogisticRegression(\n",
        "    max_iter=1000,        # Number of iterations\n",
        "    random_state=42,\n",
        "    solver='lbfgs',       # Solver algorithm\n",
        "    multi_class='multinomial'  # For multi-class (26 classes)\n",
        ")\n",
        "# Train on FULL training data (all losses visible)\n",
        "policy.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions on training set\n",
        "train_predictions = policy.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "train_error = 1 - train_accuracy\n",
        "\n",
        "print(f\"\\nPolicy trained: Logistic Regression\")\n",
        "print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Training error: {train_error:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd6f845",
      "metadata": {},
      "outputs": [],
      "source": [
        "#COMPUTE GROUND TRUTH\n",
        "\n",
        "# This is the TRUE policy value we're trying to estimate\n",
        "test_predictions = policy.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "test_error_true = 1 - test_accuracy  # This is V(π) - the ground truth!\n",
        "\n",
        "\n",
        "print(\"GROUND TRUTH (what we're trying to estimate)\")\n",
        "print(f\"True test accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"True test error (V(π)): {test_error_true:.4f}\")\n",
        "print(\"\\n This is the ground truth that we'll compare our estimates against!\")\n",
        "\n",
        "# Save policy predictions for later use\n",
        "policy_test_predictions = policy.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f9e9468",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "### STEP 4: IMPLEMENT DM, IPS, AND DR ESTIMATORS\n",
        "\n",
        "\n",
        "\n",
        "### Helper Function: Train Reward Model \n",
        "\n",
        "def train_reward_model(bandit_data, X_full, n_classes):\n",
        "    \n",
        "    contexts = bandit_data['contexts']\n",
        "    actions = bandit_data['actions']\n",
        "    rewards = bandit_data['rewards']\n",
        "    n_samples = len(contexts)\n",
        "    n_features = X_full.shape[1]\n",
        "    \n",
        "    # Create feature matrix: [context features + one-hot encoded action]\n",
        "    # For each sample: [x_1, x_2, ..., x_d, I(a=0), I(a=1), ..., I(a=k-1)]\n",
        "    X_with_actions = np.zeros((n_samples, n_features + n_classes))\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        # Copy context features\n",
        "        X_with_actions[i, :n_features] = contexts[i]\n",
        "        # Add one-hot encoded action\n",
        "        X_with_actions[i, n_features + actions[i]] = 1\n",
        "    \n",
        "    # Train to predict rewards\n",
        "    reward_model = Ridge(\n",
        "    alpha=1.0,           # Regularization strength\n",
        "    random_state=42\n",
        "    )\n",
        "    reward_model.fit(X_with_actions, rewards)\n",
        "    \n",
        "    return reward_model\n",
        "\n",
        "\n",
        "def predict_rewards(reward_model, contexts, actions, n_features, n_classes):\n",
        "    n_samples = len(contexts)\n",
        "    X_with_actions = np.zeros((n_samples, n_features + n_classes))\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        # Copy context features\n",
        "        X_with_actions[i, :n_features] = contexts[i]\n",
        "        # Add one-hot encoded action\n",
        "        X_with_actions[i, n_features + actions[i]] = 1\n",
        "    \n",
        "    predicted_rewards = reward_model.predict(X_with_actions)\n",
        "    return predicted_rewards\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0720ebe6",
      "metadata": {},
      "outputs": [],
      "source": [
        "### ESTIMATOR 1: DIRECT METHOD (DM)\n",
        "\n",
        "def estimate_dm(bandit_data, policy_actions, reward_model, n_features, n_classes):\n",
        "    contexts = bandit_data['contexts']\n",
        "    n_samples = len(contexts)\n",
        "    \n",
        "    # Predict rewards for the policy's chosen actions\n",
        "    predicted_rewards = predict_rewards(\n",
        "        reward_model, contexts, policy_actions, n_features, n_classes\n",
        "    )\n",
        "    \n",
        "    # DM estimate: average of predicted rewards\n",
        "    dm_reward_estimate = np.mean(predicted_rewards)\n",
        "    \n",
        "    # Convert reward to loss (loss = 1 - reward)\n",
        "    dm_loss_estimate = 1 - dm_reward_estimate\n",
        "    \n",
        "    return dm_loss_estimate\n",
        "\n",
        "\n",
        "\n",
        "### ESTIMATOR 2: INVERSE PROPENSITY SCORING (IPS)\n",
        "\n",
        "def estimate_ips(bandit_data, policy_actions):\n",
        "\n",
        "    actions_logged = bandit_data['actions']      # What was actually done\n",
        "    rewards = bandit_data['rewards']             # Observed rewards\n",
        "    propensities = bandit_data['propensities']   # p(a|x) from logging policy\n",
        "    n_samples = len(actions_logged)\n",
        "    \n",
        "    # Indicator: I(logged action = policy action)\n",
        "    matches = (actions_logged == policy_actions).astype(float)\n",
        "    \n",
        "    # Importance weights: I(a_logged = a_policy) / p(a_logged | x)\n",
        "    importance_weights = matches / propensities\n",
        "    \n",
        "    # IPS estimate: weighted average of observed rewards\n",
        "    weighted_rewards = importance_weights * rewards\n",
        "    ips_reward_estimate = np.mean(weighted_rewards)\n",
        "    \n",
        "    # Convert reward to loss\n",
        "    ips_loss_estimate = 1 - ips_reward_estimate\n",
        "    \n",
        "    return ips_loss_estimate\n",
        "\n",
        "\n",
        "\n",
        "### ESTIMATOR 3: DOUBLY ROBUST (DR)\n",
        "\n",
        "\n",
        "def estimate_dr(bandit_data, policy_actions, reward_model, n_features, n_classes):\n",
        "    \n",
        "    contexts = bandit_data['contexts']\n",
        "    actions_logged = bandit_data['actions']\n",
        "    rewards = bandit_data['rewards']\n",
        "    propensities = bandit_data['propensities']\n",
        "    n_samples = len(contexts)\n",
        "    \n",
        "    # Part 1: Direct Method component\n",
        "    # Predict rewards for policy's actions: r̂(x, π(x))\n",
        "    predicted_rewards_policy = predict_rewards(\n",
        "        reward_model, contexts, policy_actions, n_features, n_classes\n",
        "    )\n",
        "    \n",
        "    # Part 2: Predict rewards for logged actions: r̂(x, a_logged)\n",
        "    # This is what the model THINKS the logged action would get\n",
        "    predicted_rewards_logged = predict_rewards(\n",
        "        reward_model, contexts, actions_logged, n_features, n_classes\n",
        "    )\n",
        "    \n",
        "    # Part 3: IPS correction term\n",
        "    # Only apply correction when logged action = policy action\n",
        "    matches = (actions_logged == policy_actions).astype(float)\n",
        "    importance_weights = matches / propensities\n",
        "    \n",
        "    # Prediction error: (actual - predicted) for the logged action\n",
        "    prediction_errors = rewards - predicted_rewards_logged\n",
        "    \n",
        "    # Correction: re-weight the prediction errors\n",
        "    correction = importance_weights * prediction_errors\n",
        "    \n",
        "    # Part 4: Combine using DR formula\n",
        "    # DR = Model prediction + Correction for model errors\n",
        "    dr_rewards = predicted_rewards_policy + correction\n",
        "    dr_reward_estimate = np.mean(dr_rewards)\n",
        "    \n",
        "    # Convert reward to loss\n",
        "    dr_loss_estimate = 1 - dr_reward_estimate\n",
        "    \n",
        "    return dr_loss_estimate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "239c73cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "### TEST THE ESTIMATORS (Single Run)\n",
        "\n",
        "# Create bandit feedback from test set\n",
        "print(\"\\n Creating bandit feedback with uniform logging policy...\")\n",
        "test_bandit = create_bandit_data(\n",
        "    X_test, y_test, test_losses_full, \n",
        "    logging_policy='uniform'\n",
        ")\n",
        "print(f\"  Created {len(test_bandit['contexts'])} bandit samples\")\n",
        "print(f\"  Logging policy: uniform (p = 1/{n_classes} = {1/n_classes:.4f})\")\n",
        "\n",
        "# Train reward model on bandit data\n",
        "print(\"\\n2. Training reward model (Random Forest Regressor)...\")\n",
        "reward_model = train_reward_model(test_bandit, X_test, n_classes)\n",
        "print(f\"    Reward model trained on {len(test_bandit['contexts'])} samples\")\n",
        "\n",
        "# Get policy predictions on test contexts\n",
        "print(\"\\n3. Getting policy predictions...\")\n",
        "policy_test_actions = policy.predict(test_bandit['contexts'])\n",
        "print(f\"    Policy predictions obtained for {len(policy_test_actions)} samples\")\n",
        "\n",
        "# Compute all three estimates\n",
        "\n",
        "n_features = X_test.shape[1]\n",
        "\n",
        "dm_estimate = estimate_dm(\n",
        "    test_bandit, policy_test_actions, reward_model, n_features, n_classes\n",
        ")\n",
        "\n",
        "\n",
        "ips_estimate = estimate_ips(test_bandit, policy_test_actions)\n",
        "\n",
        "\n",
        "dr_estimate = estimate_dr(\n",
        "    test_bandit, policy_test_actions, reward_model, n_features, n_classes\n",
        ")\n",
        "\n",
        "\n",
        "# Display results\n",
        "\n",
        "print(\"RESULTS: Single Run Comparison\")\n",
        "\n",
        "print(f\"\\nGround Truth V(π):        {test_error_true:.4f}\")\n",
        "\n",
        "print(f\"Direct Method (DM):       {dm_estimate:.4f}  (error: {dm_estimate - test_error_true:+.4f})\")\n",
        "print(f\"IPS:                      {ips_estimate:.4f}  (error: {ips_estimate - test_error_true:+.4f})\")\n",
        "print(f\"Doubly Robust (DR):       {dr_estimate:.4f}  (error: {dr_estimate - test_error_true:+.4f})\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78bfb0c2",
      "metadata": {},
      "source": [
        "**Key Findings**:\n",
        "- Is that the policy value of the DR is most close to the true policy value. \n",
        "- While the IPS comes close to the DR method, the DM method seems to highly overestimate the policy value. What can we learn from this? DM method leads to poor generalization, especially in cases with context-pairs where little to no data exists, likely leading to the overestimation of the policy value that we observe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "805b68d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "### STEP 5: RUN MULTIPLE SIMULATIONS \n",
        "\n",
        "n_simulations = 100\n",
        "n_features = X_test.shape[1]\n",
        "\n",
        "# Storage for results\n",
        "dm_estimates = []\n",
        "ips_estimates = []\n",
        "dr_estimates = []\n",
        "\n",
        "print(f\"\\nRunning {n_simulations} simulations...\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "for sim in range(n_simulations):\n",
        "    # Progress indicator\n",
        "    if (sim + 1) % 50 == 0:\n",
        "        print(f\"  Completed {sim + 1}/{n_simulations} simulations...\")\n",
        "    \n",
        "    # Step 1: Create NEW bandit feedback (different random sample each time)\n",
        "    bandit_sample = create_bandit_data(\n",
        "        X_test, y_test, test_losses_full, \n",
        "        logging_policy='uniform'\n",
        "    )\n",
        "    \n",
        "    # Step 2: Train reward model on this bandit sample\n",
        "    reward_model = train_reward_model(bandit_sample, X_test, n_classes)\n",
        "    \n",
        "    # Step 3: Get policy predictions\n",
        "    policy_actions = policy.predict(bandit_sample['contexts'])\n",
        "    \n",
        "    # Step 4: Compute all three estimates\n",
        "    dm_est = estimate_dm(bandit_sample, policy_actions, reward_model, \n",
        "                         n_features, n_classes)\n",
        "    ips_est = estimate_ips(bandit_sample, policy_actions)\n",
        "    dr_est = estimate_dr(bandit_sample, policy_actions, reward_model, \n",
        "                         n_features, n_classes)\n",
        "    \n",
        "    # Step 5: Store results\n",
        "    dm_estimates.append(dm_est)\n",
        "    ips_estimates.append(ips_est)\n",
        "    dr_estimates.append(dr_est)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "dm_estimates = np.array(dm_estimates)\n",
        "ips_estimates = np.array(ips_estimates)\n",
        "dr_estimates = np.array(dr_estimates)\n",
        "\n",
        "print(f\"\\n✓ All {n_simulations} simulations completed!\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55791ee1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "### STEP 6: COMPUTE PERFORMANCE METRICS \n",
        "\n",
        "\n",
        "# Calculate bias, variance, and RMSE for each estimator\n",
        "def compute_metrics(estimates, true_value):\n",
        "\n",
        "    bias = np.mean(estimates) - true_value\n",
        "    variance = np.var(estimates, ddof=1)  # Sample variance\n",
        "    rmse = np.sqrt(bias**2 + variance)\n",
        "    \n",
        "    return bias, variance, rmse\n",
        "\n",
        "# Compute metrics for each estimator\n",
        "dm_bias, dm_var, dm_rmse = compute_metrics(dm_estimates, test_error_true)\n",
        "ips_bias, ips_var, ips_rmse = compute_metrics(ips_estimates, test_error_true)\n",
        "dr_bias, dr_var, dr_rmse = compute_metrics(dr_estimates, test_error_true)\n",
        "\n",
        "# Display results\n",
        "\n",
        "\n",
        "print(f\"{'Estimator':<20} {'Mean':<12} {'Bias':<12} {'Variance':<12} {'RMSE':<12}\")\n",
        "\n",
        "print(f\"{'Direct Method':<20} {np.mean(dm_estimates):<12.4f} {dm_bias:<12.4f} {dm_var:<12.6f} {dm_rmse:<12.4f}\")\n",
        "print(f\"{'IPS':<20} {np.mean(ips_estimates):<12.4f} {ips_bias:<12.4f} {ips_var:<12.6f} {ips_rmse:<12.4f}\")\n",
        "print(f\"{'Doubly Robust':<20} {np.mean(dr_estimates):<12.4f} {dr_bias:<12.4f} {dr_var:<12.6f} {dr_rmse:<12.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b3b804",
      "metadata": {},
      "source": [
        "**Key Findings;**\n",
        "- With the DM method we might have a very precise estimator, but again, this estimator is highly biased.\n",
        "- While the IPS is slightly less biased, it is also slightly less precise, leading the DR method to achieve best overall performance with lowest RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df64c405",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "### STEP 7: VISUALIZATIONS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "### FIGURE 1: Distribution of Estimates (Histograms)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Plot 1: Direct Method\n",
        "axes[0].hist(dm_estimates, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0].axvline(test_error_true, color='red', linestyle='--', linewidth=2, label='Ground Truth')\n",
        "axes[0].axvline(np.mean(dm_estimates), color='blue', linestyle='-', linewidth=2, label='Mean Estimate')\n",
        "axes[0].set_xlabel('Estimated Error Rate')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Direct Method (DM)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: IPS\n",
        "axes[1].hist(ips_estimates, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[1].axvline(test_error_true, color='red', linestyle='--', linewidth=2, label='Ground Truth')\n",
        "axes[1].axvline(np.mean(ips_estimates), color='green', linestyle='-', linewidth=2, label='Mean Estimate')\n",
        "axes[1].set_xlabel('Estimated Error Rate')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Inverse Propensity Scoring (IPS)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Doubly Robust\n",
        "axes[2].hist(dr_estimates, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "axes[2].axvline(test_error_true, color='red', linestyle='--', linewidth=2, label='Ground Truth')\n",
        "axes[2].axvline(np.mean(dr_estimates), color='darkred', linestyle='-', linewidth=2, label='Mean Estimate')\n",
        "axes[2].set_xlabel('Estimated Error Rate')\n",
        "axes[2].set_ylabel('Frequency')\n",
        "axes[2].set_title('Doubly Robust (DR)')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f'Distribution of Estimates Across {n_simulations} Simulations', \n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae843ba4",
      "metadata": {},
      "outputs": [],
      "source": [
        "### FIGURE 2: Bias and RMSE Comparison (Bar Charts)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "estimators = ['DM', 'IPS', 'DR']\n",
        "biases = [dm_bias, ips_bias, dr_bias]\n",
        "rmses = [dm_rmse, ips_rmse, dr_rmse]\n",
        "colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
        "\n",
        "# Plot 1: Bias\n",
        "axes[0].bar(estimators, biases, color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[0].axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
        "axes[0].set_ylabel('Bias', fontsize=12)\n",
        "axes[0].set_title('Bias: Mean Estimate - Ground Truth', fontsize=13, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (est, bias) in enumerate(zip(estimators, biases)):\n",
        "    axes[0].text(i, bias + 0.002, f'{bias:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot 2: RMSE\n",
        "axes[1].bar(estimators, rmses, color=colors, edgecolor='black', linewidth=1.5)\n",
        "axes[1].set_ylabel('RMSE (Root Mean Squared Error)', fontsize=12)\n",
        "axes[1].set_title('RMSE: Overall Estimation Error', fontsize=13, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (est, rmse) in enumerate(zip(estimators, rmses)):\n",
        "    axes[1].text(i, rmse + 0.002, f'{rmse:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.suptitle(f'Performance Comparison Across {n_simulations} Simulations', \n",
        "             fontsize=14, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d4bb72",
      "metadata": {},
      "outputs": [],
      "source": [
        "### FIGURE 3: Bias-Variance Decomposition\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "estimators = ['Direct\\nMethod', 'Inverse\\nPropensity\\nScoring', 'Doubly\\nRobust']\n",
        "biases_squared = [dm_bias**2, ips_bias**2, dr_bias**2]\n",
        "variances = [dm_var, ips_var, dr_var]\n",
        "\n",
        "x = np.arange(len(estimators))\n",
        "width = 0.35\n",
        "\n",
        "# Stacked bar chart\n",
        "bars1 = ax.bar(x, biases_squared, width, label='Bias²', color='coral', edgecolor='black')\n",
        "bars2 = ax.bar(x, variances, width, bottom=biases_squared, label='Variance', \n",
        "               color='steelblue', edgecolor='black')\n",
        "\n",
        "ax.set_ylabel('Error Contribution', fontsize=12)\n",
        "ax.set_title('Bias-Variance Decomposition: RMSE² = Bias² + Variance', \n",
        "             fontsize=13, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(estimators)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add RMSE² labels on top\n",
        "for i, rmse in enumerate([dm_rmse, ips_rmse, dr_rmse]):\n",
        "    total_height = biases_squared[i] + variances[i]\n",
        "    ax.text(i, total_height + 0.0002, f'RMSE²={rmse**2:.6f}', \n",
        "            ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "939eb25a",
      "metadata": {},
      "outputs": [],
      "source": [
        "### FIGURE 4: Convergence Plot (Shows estimates over time)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Calculate cumulative means\n",
        "dm_cumulative = np.cumsum(dm_estimates) / np.arange(1, len(dm_estimates) + 1)\n",
        "ips_cumulative = np.cumsum(ips_estimates) / np.arange(1, len(ips_estimates) + 1)\n",
        "dr_cumulative = np.cumsum(dr_estimates) / np.arange(1, len(dr_estimates) + 1)\n",
        "\n",
        "# Plot cumulative means\n",
        "ax.plot(dm_cumulative, label='DM', color='blue', linewidth=2, alpha=0.7)\n",
        "ax.plot(ips_cumulative, label='IPS', color='green', linewidth=2, alpha=0.7)\n",
        "ax.plot(dr_cumulative, label='DR', color='red', linewidth=2, alpha=0.7)\n",
        "\n",
        "# Ground truth reference line\n",
        "ax.axhline(test_error_true, color='black', linestyle='--', linewidth=2, \n",
        "           label=f'Ground Truth = {test_error_true:.4f}')\n",
        "\n",
        "ax.set_xlabel('Number of Simulations', fontsize=12)\n",
        "ax.set_ylabel('Cumulative Mean Estimate', fontsize=12)\n",
        "ax.set_title('Convergence of Estimators (Running Average)', fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4320c802",
      "metadata": {},
      "source": [
        "**Key Findings:**\n",
        "- DM does not converge to the truth.\n",
        "- IPS and DR are consistent; i.e., they do converge to the truth over simulations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd36eb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "### FIGURE 5: Summary Table \n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Create summary table\n",
        "table_data = [\n",
        "    ['Estimator', 'Mean', 'Bias', 'Variance', 'RMSE', 'Description'],\n",
        "    ['Direct Method', f'{np.mean(dm_estimates):.4f}', f'{dm_bias:+.4f}', \n",
        "     f'{dm_var:.6f}', f'{dm_rmse:.4f}', 'Model-based (low variance, potential bias)'],\n",
        "    ['IPS', f'{np.mean(ips_estimates):.4f}', f'{ips_bias:+.4f}', \n",
        "     f'{ips_var:.6f}', f'{ips_rmse:.4f}', 'Importance-weighted (unbiased, high variance)'],\n",
        "    ['Doubly Robust', f'{np.mean(dr_estimates):.4f}', f'{dr_bias:+.4f}', \n",
        "     f'{dr_var:.6f}', f'{dr_rmse:.4f}', 'Combined approach (best overall)']\n",
        "]\n",
        "\n",
        "table = ax.table(cellText=table_data, cellLoc='center', loc='center',\n",
        "                 colWidths=[0.15, 0.12, 0.12, 0.12, 0.12, 0.37])\n",
        "\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2)\n",
        "\n",
        "# Style header row\n",
        "for i in range(6):\n",
        "    cell = table[(0, i)]\n",
        "    cell.set_facecolor('#40466e')\n",
        "    cell.set_text_props(weight='bold', color='white')\n",
        "\n",
        "# Style data rows with alternating colors\n",
        "colors_row = ['#f1f1f2', '#ffffff', '#f1f1f2']\n",
        "for i in range(1, 4):\n",
        "    for j in range(6):\n",
        "        cell = table[(i, j)]\n",
        "        cell.set_facecolor(colors_row[i-1])\n",
        "\n",
        "plt.suptitle(\n",
        "    f'Summary: Offline Policy Evaluation Results ({n_simulations} simulations)\\n'\n",
        "    f'Ground Truth V(π) = {test_error_true:.4f}',\n",
        "    fontsize=13, fontweight='bold', y=0.98\n",
        ")\n",
        "\n",
        "# No saving — show only\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "964e0ef8",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Experiment with Different Epsilon Values\n",
        "\n",
        "def run_epsilon_experiment(epsilon_values, n_simulations=100):\n",
        "    \n",
        "    results = {\n",
        "        'epsilon': [],\n",
        "        'dm_bias': [], 'dm_var': [], 'dm_rmse': [],\n",
        "        'ips_bias': [], 'ips_var': [], 'ips_rmse': [],\n",
        "        'dr_bias': [], 'dr_var': [], 'dr_rmse': []\n",
        "    }\n",
        "    \n",
        "    n_features = X_test.shape[1]\n",
        "    \n",
        "    for eps in epsilon_values:\n",
        "        \n",
        "        print(f\"Testing ε = {eps:.2f} ({n_simulations} simulations)\")\n",
        "        \n",
        "        \n",
        "        dm_estimates = []\n",
        "        ips_estimates = []\n",
        "        dr_estimates = []\n",
        "        \n",
        "        for sim in range(n_simulations):\n",
        "            if (sim + 1) % 25 == 0:\n",
        "                print(f\"  Progress: {sim + 1}/{n_simulations}...\", end='\\r')\n",
        "            \n",
        "            # Create bandit feedback with epsilon-greedy logging\n",
        "            bandit_sample = create_bandit_data(\n",
        "                X_test, y_test, test_losses_full,\n",
        "                logging_policy='epsilon_greedy',\n",
        "                epsilon=eps\n",
        "            )\n",
        "            \n",
        "            # Train reward model\n",
        "            reward_model = train_reward_model(bandit_sample, X_test, n_classes)\n",
        "            \n",
        "            # Get policy predictions\n",
        "            policy_actions = policy.predict(bandit_sample['contexts'])\n",
        "            \n",
        "            # Compute estimates\n",
        "            dm_est = estimate_dm(bandit_sample, policy_actions, reward_model,\n",
        "                               n_features, n_classes)\n",
        "            ips_est = estimate_ips(bandit_sample, policy_actions)\n",
        "            dr_est = estimate_dr(bandit_sample, policy_actions, reward_model,\n",
        "                               n_features, n_classes)\n",
        "            \n",
        "            dm_estimates.append(dm_est)\n",
        "            ips_estimates.append(ips_est)\n",
        "            dr_estimates.append(dr_est)\n",
        "        \n",
        "        # Convert to arrays\n",
        "        dm_estimates = np.array(dm_estimates)\n",
        "        ips_estimates = np.array(ips_estimates)\n",
        "        dr_estimates = np.array(dr_estimates)\n",
        "        \n",
        "        # Compute metrics\n",
        "        dm_bias, dm_var, dm_rmse = compute_metrics(dm_estimates, test_error_true)\n",
        "        ips_bias, ips_var, ips_rmse = compute_metrics(ips_estimates, test_error_true)\n",
        "        dr_bias, dr_var, dr_rmse = compute_metrics(dr_estimates, test_error_true)\n",
        "        \n",
        "        # Store results\n",
        "        results['epsilon'].append(eps)\n",
        "        results['dm_bias'].append(dm_bias)\n",
        "        results['dm_var'].append(dm_var)\n",
        "        results['dm_rmse'].append(dm_rmse)\n",
        "        results['ips_bias'].append(ips_bias)\n",
        "        results['ips_var'].append(ips_var)\n",
        "        results['ips_rmse'].append(ips_rmse)\n",
        "        results['dr_bias'].append(dr_bias)\n",
        "        results['dr_var'].append(dr_var)\n",
        "        results['dr_rmse'].append(dr_rmse)\n",
        "        \n",
        "        print(f\"\\n  ✓ Completed ε = {eps:.2f}\")\n",
        "        print(f\"    IPS Variance: {ips_var:.6f}\")\n",
        "        print(f\"    DR RMSE: {dr_rmse:.4f}\")\n",
        "    \n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89b52099",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: STUDENTS CAN MODIFY THESE VALUES!\n",
        "epsilon_values_to_test = [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]  # try for 0.1, 0.3, 0.5, 0.7, 1.0\n",
        "n_sims = 100  # Reduce for faster testing, you can try for 200, 400, 500 etc\n",
        "results = run_epsilon_experiment(epsilon_values_to_test, n_simulations=n_sims)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "552ce805",
      "metadata": {},
      "outputs": [],
      "source": [
        "### TASK 2: Visualize Results\n",
        "# EXPERIMENT:\n",
        "# Change epsilon values to see how overlap affects DM, IPS, and DR estimators.\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Variance vs Epsilon\n",
        "axes[0, 0].plot(results['epsilon'], results['dm_var'], 'o-', \n",
        "                label='DM', linewidth=2, markersize=8, color='skyblue')\n",
        "axes[0, 0].plot(results['epsilon'], results['ips_var'], 's-', \n",
        "                label='IPS', linewidth=2, markersize=8, color='lightgreen')\n",
        "axes[0, 0].plot(results['epsilon'], results['dr_var'], '^-', \n",
        "                label='DR', linewidth=2, markersize=8, color='lightcoral')\n",
        "axes[0, 0].set_xlabel('Epsilon (ε)', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Variance', fontsize=11)\n",
        "axes[0, 0].set_title('Variance vs Exploration Rate', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].set_yscale('log')  # Log scale to see differences better\n",
        "\n",
        "# Plot 2: RMSE vs Epsilon\n",
        "axes[0, 1].plot(results['epsilon'], results['dm_rmse'], 'o-', \n",
        "                label='DM', linewidth=2, markersize=8, color='skyblue')\n",
        "axes[0, 1].plot(results['epsilon'], results['ips_rmse'], 's-', \n",
        "                label='IPS', linewidth=2, markersize=8, color='lightgreen')\n",
        "axes[0, 1].plot(results['epsilon'], results['dr_rmse'], '^-', \n",
        "                label='DR', linewidth=2, markersize=8, color='lightcoral')\n",
        "axes[0, 1].set_xlabel('Epsilon (ε)', fontsize=11)\n",
        "axes[0, 1].set_ylabel('RMSE', fontsize=11)\n",
        "axes[0, 1].set_title('RMSE vs Exploration Rate', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=10)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Bias vs Epsilon\n",
        "axes[1, 0].plot(results['epsilon'], results['dm_bias'], 'o-', \n",
        "                label='DM', linewidth=2, markersize=8, color='skyblue')\n",
        "axes[1, 0].plot(results['epsilon'], results['ips_bias'], 's-', \n",
        "                label='IPS', linewidth=2, markersize=8, color='lightgreen')\n",
        "axes[1, 0].plot(results['epsilon'], results['dr_bias'], '^-', \n",
        "                label='DR', linewidth=2, markersize=8, color='lightcoral')\n",
        "axes[1, 0].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
        "axes[1, 0].set_xlabel('Epsilon (ε)', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Bias', fontsize=11)\n",
        "axes[1, 0].set_title('Bias vs Exploration Rate', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].legend(fontsize=10)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Average Propensity Score vs Epsilon\n",
        "avg_propensities = []\n",
        "for eps in results['epsilon']:\n",
        "    sample = create_bandit_data(\n",
        "        X_test[:1000], y_test[:1000], test_losses_full[:1000],\n",
        "        logging_policy='epsilon_greedy', epsilon=eps\n",
        "    )\n",
        "    avg_propensities.append(np.mean(sample['propensities']))\n",
        "\n",
        "axes[1, 1].plot(results['epsilon'], avg_propensities, 'o-', \n",
        "                linewidth=2, markersize=8, color='purple')\n",
        "axes[1, 1].set_xlabel('Epsilon (ε)', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Average Propensity Score', fontsize=11)\n",
        "axes[1, 1].set_title('Overlap: Average p(a|x)', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].axhline(1/n_classes, color='red', linestyle='--', \n",
        "                   label=f'Uniform = {1/n_classes:.4f}', alpha=0.7)\n",
        "axes[1, 1].legend(fontsize=10)\n",
        "\n",
        "plt.suptitle('Impact of Exploration Rate (Epsilon) on Estimator Performance', \n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4157336",
      "metadata": {},
      "source": [
        "**Key Findings:**\n",
        "- DM is not stable with increasing ε, with RMSE increasing linearly from ε=0.3 onwards.\n",
        "- DM method is highly sensitive to the exploration rate, where bias is positive and linearly related to the values of the epsilon.\n",
        "- IPS has high variance at low overlap.\n",
        "- DR is the most robust estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "503d2018",
      "metadata": {},
      "source": [
        "Now, we move on to a new (simulated) dataset: Website visitations from ads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc07131a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tasks to run \n",
        "\n",
        "\n",
        "# Data Generation \n",
        "def generate_ad_data(n=5000, seed=42, epsilon=0.3):\n",
        "    \"\"\"Generate synthetic ad campaign data\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    age = np.random.normal(35, 12, n)\n",
        "    income = np.random.gamma(shape=2, scale=30000, size=n)\n",
        "    time_on_site = np.random.exponential(scale=5, size=n)\n",
        "    previous_purchases = np.random.poisson(lam=2, size=n)\n",
        "    engagement_score = np.random.beta(2, 5, n) * 100\n",
        "    \n",
        "    propensity_score = epsilon + (1 - epsilon) * (engagement_score > 50).astype(float)\n",
        "    treatment = np.random.binomial(1, propensity_score)\n",
        "    optimal_treatment = ((income > 50000) & (engagement_score > 60)).astype(int)\n",
        "    \n",
        "    base_value = (0.5 * (age - 35) + 0.001 * income + 2 * time_on_site + \n",
        "                  5 * previous_purchases + 0.3 * engagement_score + \n",
        "                  np.random.normal(0, 10, n))\n",
        "    \n",
        "    treatment_effect = (20 * (income > 50000) + 15 * (engagement_score > 60) + \n",
        "                       -10 * (age < 25) + np.random.normal(0, 5, n))\n",
        "    \n",
        "    Y = np.maximum(base_value + treatment * treatment_effect, 0)\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'age': age, 'income': income, 'time_on_site': time_on_site,\n",
        "        'previous_purchases': previous_purchases, 'engagement_score': engagement_score,\n",
        "        'treatment': treatment, 'propensity_score': propensity_score,\n",
        "        'outcome': Y, 'optimal_treatment': optimal_treatment\n",
        "    })\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Quick view of generated data\n",
        "sample_df = generate_ad_data(n=100, seed=42, epsilon=0.3)\n",
        "print(sample_df.head(20))\n",
        "\n",
        "\n",
        "class DirectMethodEstimator:\n",
        "    \"\"\"Direct Method: Learn E[Y|X,A], then optimize policy\"\"\"\n",
        "    \n",
        "    def __init__(self, model=None):\n",
        "        if model is None:\n",
        "            self.model = RandomForestRegressor(n_estimators=50, max_depth=3, random_state=42)\n",
        "        else:\n",
        "            self.model = model\n",
        "        self.feature_cols = ['age', 'income', 'time_on_site', 'previous_purchases', 'engagement_score']\n",
        "    \n",
        "    def fit(self, df):\n",
        "        X = df[self.feature_cols].reset_index(drop=True)\n",
        "        X_with_treatment = pd.concat([X, pd.Series(df['treatment'].values, name='treatment')], axis=1)\n",
        "        self.model.fit(X_with_treatment, df['outcome'].values)\n",
        "    \n",
        "    def predict_policy(self, df):\n",
        "        X = df[self.feature_cols].reset_index(drop=True)\n",
        "        X_treat_0 = pd.concat([X, pd.Series(0, index=X.index, name='treatment')], axis=1)\n",
        "        X_treat_1 = pd.concat([X, pd.Series(1, index=X.index, name='treatment')], axis=1)\n",
        "        y_pred_0 = self.model.predict(X_treat_0)\n",
        "        y_pred_1 = self.model.predict(X_treat_1)\n",
        "        return (y_pred_1 > y_pred_0).astype(int)\n",
        "    \n",
        "    def estimate_value(self, df):\n",
        "        policy = self.predict_policy(df)\n",
        "        X = df[self.feature_cols].reset_index(drop=True)\n",
        "        X_with_policy = pd.concat([X, pd.Series(policy, index=X.index, name='treatment')], axis=1)\n",
        "        return self.model.predict(X_with_policy).mean()\n",
        "\n",
        "\n",
        "class IPWEstimator:\n",
        "    \"\"\"Inverse Propensity Weighting: Reweight observations\"\"\"\n",
        "    \n",
        "    def __init__(self, policy_model=None):\n",
        "        if policy_model is None:\n",
        "            self.policy_model = LogisticRegression(max_iter=500)\n",
        "        else:\n",
        "            self.policy_model = policy_model\n",
        "        self.feature_cols = ['age', 'income', 'time_on_site', 'previous_purchases', 'engagement_score']\n",
        "    \n",
        "    def fit(self, df):\n",
        "        X = df[self.feature_cols].reset_index(drop=True)\n",
        "        weights = np.where(df['treatment'].values == 1, \n",
        "                          df['outcome'].values / df['propensity_score'].values,\n",
        "                          df['outcome'].values / (1 - df['propensity_score'].values))\n",
        "        self.policy_model.fit(X, df['treatment'].values, sample_weight=np.abs(weights))\n",
        "    \n",
        "    def predict_policy(self, df):\n",
        "        X = df[self.feature_cols].reset_index(drop=True)\n",
        "        return self.policy_model.predict(X)\n",
        "    \n",
        "    def estimate_value(self, df):\n",
        "        policy = self.predict_policy(df)\n",
        "        weights = np.where(df['treatment'].values == policy,\n",
        "                          1 / np.where(df['treatment'].values == 1, df['propensity_score'].values, \n",
        "                                      1 - df['propensity_score'].values), 0)\n",
        "        return (weights * df['outcome'].values).sum() / len(df)\n",
        "\n",
        "\n",
        "class DoublyRobustEstimator:\n",
        "    \"\"\"Doubly Robust: Combines DM and IPW\"\"\"\n",
        "    \n",
        "    def __init__(self, outcome_model=None, policy_model=None):\n",
        "        if outcome_model is None:\n",
        "            self.outcome_model = RandomForestRegressor(n_estimators=50, max_depth=3, random_state=42)\n",
        "        else:\n",
        "            self.outcome_model = outcome_model\n",
        "        if policy_model is None:\n",
        "            self.policy_model = LogisticRegression(max_iter=500)\n",
        "        else:\n",
        "            self.policy_model = policy_model\n",
        "        self.feature_cols = ['age', 'income', 'time_on_site', 'previous_purchases', 'engagement_score']\n",
        "    \n",
        "    def fit(self, df):\n",
        "        X = df[self.feature_cols].reset_index(drop=True)\n",
        "        X_with_treatment = pd.concat([X, pd.Series(df['treatment'].values, name='treatment')], axis=1)\n",
        "        self.outcome_model.fit(X_with_treatment, df['outcome'].values)\n",
        "        self.policy_model.fit(X, df['treatment'].values)\n",
        "    \n",
        "    def predict_policy(self, df):\n",
        "        X = df[self.feature_cols].reset_index(drop=True)\n",
        "        X_treat_0 = pd.concat([X, pd.Series(0, index=X.index, name='treatment')], axis=1)\n",
        "        X_treat_1 = pd.concat([X, pd.Series(1, index=X.index, name='treatment')], axis=1)\n",
        "        y_pred_0 = self.outcome_model.predict(X_treat_0)\n",
        "        y_pred_1 = self.outcome_model.predict(X_treat_1)\n",
        "        return (y_pred_1 > y_pred_0).astype(int)\n",
        "    \n",
        "    def estimate_value(self, df):\n",
        "        policy = self.predict_policy(df)\n",
        "        X = df[self.feature_cols].reset_index(drop=True)\n",
        "        X_with_policy = pd.concat([X, pd.Series(policy, index=X.index, name='treatment')], axis=1)\n",
        "        y_pred = self.outcome_model.predict(X_with_policy)\n",
        "        \n",
        "        weights = np.where(df['treatment'].values == policy,\n",
        "                          1 / np.where(df['treatment'].values == 1, df['propensity_score'].values, \n",
        "                                      1 - df['propensity_score'].values), 0)\n",
        "        return (y_pred + weights * (df['outcome'].values - y_pred)).mean()\n",
        "\n",
        "\n",
        "def run_experiment(data_fn, n, reps, n_estimators=50, methods=['DM', 'IPW', 'DR']):\n",
        "    \"\"\"Run experiment comparing policy learning methods\"\"\"\n",
        "    results = []\n",
        "    oracle_values = []\n",
        "    \n",
        "    for rep in range(reps):\n",
        "        df = data_fn(n=n, seed=42 + rep)\n",
        "        train_df, test_df = train_test_split(df, test_size=0.3, random_state=rep)\n",
        "        \n",
        "        estimators = {}\n",
        "        if 'DM' in methods:\n",
        "            estimators['DM'] = DirectMethodEstimator(\n",
        "                model=RandomForestRegressor(n_estimators=n_estimators, max_depth=3, random_state=rep)\n",
        "            )\n",
        "        if 'IPW' in methods:\n",
        "            estimators['IPW'] = IPWEstimator()\n",
        "        if 'DR' in methods:\n",
        "            estimators['DR'] = DoublyRobustEstimator(\n",
        "                outcome_model=RandomForestRegressor(n_estimators=n_estimators, max_depth=3, random_state=rep)\n",
        "            )\n",
        "        \n",
        "        for name, estimator in estimators.items():\n",
        "            estimator.fit(train_df)\n",
        "            value = estimator.estimate_value(test_df)\n",
        "            results.append({'Method': name, 'Rep': rep, 'Value': value})\n",
        "        \n",
        "        optimal_policy = test_df['optimal_treatment'].values\n",
        "        X = test_df[['age', 'income', 'time_on_site', 'previous_purchases', 'engagement_score']].reset_index(drop=True)\n",
        "        X_with_opt = pd.concat([X, pd.Series(optimal_policy, name='treatment')], axis=1)\n",
        "        dm_temp = DirectMethodEstimator(model=RandomForestRegressor(n_estimators=50, random_state=rep))\n",
        "        dm_temp.fit(df)\n",
        "        oracle_values.append(dm_temp.model.predict(X_with_opt).mean())\n",
        "    \n",
        "    results_df = pd.DataFrame(results)\n",
        "    oracle_mean = np.mean(oracle_values)\n",
        "    \n",
        "    summary = results_df.groupby('Method')['Value'].agg(['mean', 'std', 'var']).reset_index()\n",
        "    summary['Bias_sq'] = (summary['mean'] - oracle_mean) ** 2\n",
        "    summary['Oracle'] = oracle_mean\n",
        "    summary.columns = ['Method', 'Mean_Value', 'Std_Dev', 'Variance', 'Bias_sq', 'Oracle']\n",
        "    \n",
        "    return summary\n",
        "\n",
        "print(\" Functions and classes loaded successfully!\")\n",
        "print(\"Ready to run experiments\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c886a79",
      "metadata": {},
      "outputs": [],
      "source": [
        "#  TASK 1 - BASELINE COMPARISON\n",
        "#  INSTRUCTIONS: Run this cell as-is to see baseline results\n",
        "# No modifications needed!\n",
        "\n",
        "print(\"   • Sample size: 5000\")\n",
        "print(\"   • Exploration rate: 0.3\") \n",
        "print(\"   • Model complexity: 50 trees\\n\")\n",
        "\n",
        "results_task1 = run_experiment(\n",
        "    data_fn=lambda n, seed: generate_ad_data(n=n, seed=seed, epsilon=0.3),\n",
        "    n=5000,\n",
        "    reps=10\n",
        ")\n",
        "\n",
        "print(\"\\nRESULTS:\")\n",
        "print(results_task1.to_string(index=False))\n",
        "\n",
        "\n",
        "\n",
        "# WHAT TO OBSERVE:\n",
        "print(\"• Mean_Value: How close is each method to Policy Value?\")\n",
        "print(\"• Bias_sq: Which method is most accurate on average?\")\n",
        "print(\"• Variance: Which method is most stable across runs?\")\n",
        "print(\"• DR should show: Moderate bias, moderate variance (balanced!)\")\n",
        "\n",
        "\n",
        "\n",
        "# Which method gets closest to the Policy value?\")\n",
        "# Does any method have both low bias AND low variance?\")\n",
        "# What's the tradeoff between DM and IPW?\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d956adce",
      "metadata": {},
      "outputs": [],
      "source": [
        "#  TASK 2 - EXPLORATION RATE SENSITIVITY\n",
        "\n",
        "#  INSTRUCTIONS: \n",
        "# 1. Run this cell with epsilon=0.1\n",
        "# 2. Observe the results (especially IPW variance!)\n",
        "# 3. Change epsilon to 0.5 and re-run\n",
        "# 4. Compare the two results\n",
        "\n",
        "# MODIFY THIS PARAMETER:\n",
        "epsilon_to_test = 0.5  # Try: 0.1, 0.3, 0.5\n",
        "\n",
        "\n",
        "print(f\"\\n Testing with exploration rate = {epsilon_to_test}\")\n",
        "print(f\"   • epsilon={epsilon_to_test} means {epsilon_to_test*100:.0f}% random, {(1-epsilon_to_test)*100:.0f}% targeted\")\n",
        "print(f\"   • Sample size: 5000\")\n",
        "print(f\"   • Model complexity: 50 trees\\n\")\n",
        "\n",
        "results_task2 = run_experiment(\n",
        "    data_fn=lambda n, seed: generate_ad_data(n=n, seed=seed, epsilon=epsilon_to_test),\n",
        "    n=5000,\n",
        "    reps=10\n",
        ")\n",
        "\n",
        "print(\"\\n RESULTS:\")\n",
        "print(results_task2.to_string(index=False))\n",
        "\n",
        "# What to observe\n",
        "# if epsilon_to_test < 0.2: \n",
        "   \n",
        "# if epsilon_to_test > 0.4:\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c967752",
      "metadata": {},
      "outputs": [],
      "source": [
        "#TASK 3 - MODEL COMPLEXITY SENSITIVITY\n",
        "# INSTRUCTIONS:\n",
        "# 1. Run this cell with n_estimators=10 (simple model)\n",
        "# 2. Observe DM's high bias\n",
        "# 3. Change to n_estimators=200 (complex model) and re-run\n",
        "# 4. Compare how DM and DR are affected\n",
        "\n",
        "#  MODIFY THIS PARAMETER:\n",
        "n_estimators_to_test = 200  # Try: 10, 50, 200\n",
        "\n",
        "\n",
        "print(f\"\\n Testing with {n_estimators_to_test} trees in Random Forest\")\n",
        "print(f\"   • Sample size: 5000\")\n",
        "print(f\"   • Exploration rate: 0.3\\n\")\n",
        "\n",
        "results_task3 = run_experiment(\n",
        "    data_fn=lambda n, seed: generate_ad_data(n=n, seed=seed, epsilon=0.3),  # we recommend to modify these values as well\n",
        "    n=5000,\n",
        "    reps=10,\n",
        "    n_estimators=n_estimators_to_test\n",
        ")\n",
        "\n",
        "print(results_task3.to_string(index=False))\n",
        "\n",
        "# What to tobserve\n",
        "# n_estimators=10:  DM Bias_sq will be HIGH (100-500)\n",
        "#                   Why? Can't capture treatment heterogeneity\n",
        "# n_estimators=200: DM Bias_sq will be LOW (10-50)\n",
        "#                   Why? Complex model captures patterns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7860206",
      "metadata": {},
      "outputs": [],
      "source": [
        "#  TASK 4 - SAMPLE SIZE SENSITIVITY\n",
        "# INSTRUCTIONS:\n",
        "# 1. Run this cell with n=1000 (small sample)\n",
        "# 2. Observe high variance for ALL methods\n",
        "# 3. Change to n=10000 (large sample) and re-run\n",
        "# 4. See how variance shrinks for everyone\n",
        "\n",
        "# MODIFY THIS PARAMETER:\n",
        "n_to_test = 10000  # Try: 1000, 5000, 10000\n",
        "\n",
        "\n",
        "results_task4 = run_experiment(\n",
        "    data_fn=lambda n, seed: generate_ad_data(n=n, seed=seed, epsilon=0.3),\n",
        "    n=n_to_test,\n",
        "    reps=10\n",
        ")\n",
        "\n",
        "print(\"\\n RESULTS:\")\n",
        "print(results_task4.to_string(index=False))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ff8f68",
      "metadata": {},
      "source": [
        "**Key take-aways from exercise:**\n",
        "- With poor overlap:\n",
        "    -  Variance in IPW explodes; DR inherits this instability\n",
        "    -  DM extrapolates and bias increases\n",
        "- With better overlap:\n",
        "    -  Variance in DM changes little\n",
        "    -  IPS stabilizes and DR improves and gets closer to the oracle\n",
        "- With more estimators:\n",
        "    -  DM/DR improve up to a certain point, then they risk overfitting\n",
        "- With higher sample size: \n",
        "    -  Variance decreases for all methods as it is negatively related to sample size\n",
        "    -  Bias does not disappear if propensities are extreme or if model is misspecified"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f25c27",
      "metadata": {},
      "source": [
        "**Suggestions for students as they move forward:**\n",
        "- For now, we have done **policy evaluation**. On your own, you can try doing policy optimization. Dudik et al. (2011) recommend the following steps for **policy optimization**:\n",
        "    - **Step 1**: Split dataset into training and test split\n",
        "    - **Step 2**: Construct a partially labelled dataset; i.e., the bandit data that we created above\n",
        "    - **Steo 3**: Use DR and IPS to impute revealed losses\n",
        "    - **Step 4**: Train cost-sensitive multiclass classification algorithms for IPS and DR (see Dudik et al., 2011 for my info)\n",
        "    - **Step 5**: Evaluate learned classifiers on test set and obtain classification error. \n",
        "- UCI has many different datasets for classifications. In Dudik et al., they used many more with varying n."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e2f185",
      "metadata": {},
      "source": [
        "## Source:\n",
        "- Dudik,M., Langford, J., & Li, L. (2011). \"Doubly Robust Policy Evaluation and Learning.\" In *International Conference on Machine Learning,* 1049-1104. PMLR"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
